## OpenAI GPT 3.5 Finetuning:

### Procedure:
1. Converting PDF file into chunks (```PDF_Chunking.ipynb```)
2. Generating Q&A pairs to be used for finetuning (```Generate Q&A Pairs using GPT-3.5.ipynb```)
3. Finetuning GPT 3.5 (```GPT-3.5-Turbo Finetuning.ipynb```)

#### PDF_Chunking.ipynb usage:
1. Run this notebook in Google Colab (ran into issues with tesseract-ocr locally)
2. Upload input PDF file onto the Colab runtime
2. Save the output ```{PDF_NAME}_questions.jsonl``` from this notebook in the ```raw_questions``` directory

#### Generate Q&A Pairs using GPT-3.5.ipynb usage:
1. Ensure that there is the raw questions jsonl file in the ```raw_questions``` directory
2. Run this notebook locally using the pdf_chunk env i.e. ```conda activate pdf_chunk```
3. The Q&A pairs output will be saved in the ```qapairs``` directory

#### GPT-3.5-Turbo Finetuning.ipynb usage:<br><br>
For finetuning:
1. Ensure that there is the unformatted Q&A pairs jsonl file (e.g. qapairs/qapairs-2023-09-14.jsonl) in the ```qapairs``` directory OR <br>
the formatted Q&A pairs jsonl files (e.g. finetuning/qapairs-2023-09-14.jsonl-train.jsonl and finetuning/qapairs-2023-09-14.jsonl-validation.jsonl) in the ```finetuning``` directory
<br>

For model inference of a finetuned model:
1. Ensure that there is the file and job ids file (e.g. finetuning/file_and_job_ids-2023-09-19) in the ```finetuning``` directory.

#### Directories:
- ```raw_questions```: used for storing raw questions obtained after PDF chunking
- ```qapairs```: used for storing generated Q&A pairs generated by GPT3.5
- ```finetuning```: used for storing finetuning train and validation dataset and file/job ids of finetunded models
